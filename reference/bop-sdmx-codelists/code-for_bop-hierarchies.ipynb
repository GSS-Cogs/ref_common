{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from gssutils import *\n",
    "from rdflib import Graph, Literal, RDF, URIRef\n",
    "import rdflib\n",
    "from rdflib.namespace import OWL, VOID, DCTERMS, RDF, RDFS, SKOS, XSD, FOAF, DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-b76a53e191f4>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  are['Notation'][are['Label'] == 'Namibia'] = 'NA' # Namibia Notation comes through as a NaN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Notation</th>\n",
       "      <th>Parent Notation</th>\n",
       "      <th>Sort Priority</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not allocated/ unspecified</td>\n",
       "      <td>_X</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not applicable</td>\n",
       "      <td>_Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMF (International Monetary Fund)</td>\n",
       "      <td>1C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WTO (World Trade Organisation)</td>\n",
       "      <td>1D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IBRD (International Bank for Reconstruction an...</td>\n",
       "      <td>1E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Label Notation  \\\n",
       "0                         Not allocated/ unspecified       _X   \n",
       "1                                     Not applicable       _Z   \n",
       "4                  IMF (International Monetary Fund)       1C   \n",
       "5                     WTO (World Trade Organisation)       1D   \n",
       "6  IBRD (International Bank for Reconstruction an...       1E   \n",
       "\n",
       "   Parent Notation  Sort Priority Description  \n",
       "0              NaN              1         NaN  \n",
       "1              NaN              2         NaN  \n",
       "4              NaN              5         NaN  \n",
       "5              NaN              6         NaN  \n",
       "6              NaN              7         NaN  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "are = pd.read_csv('ref_common/reference/codelists/cl-area.csv')\n",
    "are['Notation'][are['Label'] == 'Namibia'] = 'NA' # Namibia Notation comes through as a NaN\n",
    "are2 = are[are['Description'].notnull()]\n",
    "are3 = are[are['Description'].isna()]\n",
    "are3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Persian Gulf Economies (CDIS),S35E,,718,S35E = BH + IQ + IR + KW + OM + QA + SA + AE + A35E9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_thing_equals_other_things(descr, lab):\n",
    "    #### Example = 4AA=4B+4C+4D+4E+4G+4H+4I+4J+4R+4T+4U+4V+4S\n",
    "    meth_dat = pd.DataFrame(columns=['Narrower', 'Main', 'Main Label'])\n",
    "    s1 = descr.split('=', 1) # Split into left side of = sign and right\n",
    "    s2 = s1[1].split('+')    # split the right side by +\n",
    "    s1 = s1[0]\n",
    "    for r in s2:\n",
    "        meth_dat = pd.concat([meth_dat, pd.DataFrame({'Narrower':[r.strip()], 'Main':[s1.strip()], 'Main Label':[lab]})], sort=False)\n",
    "    return meth_dat\n",
    "\n",
    "\n",
    "def one_equals_others_plus_some_minus_some(descr, lab):\n",
    "    #### Example = B5 = I8 + (BG + CZ + DK + GB + HR + HU + PL + RO + SE+ 4A + B09) -U29\n",
    "    meth_dat = pd.DataFrame(columns=['Narrower', 'Main', 'Main Label'])\n",
    "    s1 = descr.split('-', 1) \n",
    "    s1 = s1[0].split('=', 1)\n",
    "    s1[1] = s1[1].replace('(','').replace(')','')\n",
    "    s2 = s1[1].split('+')\n",
    "    for r in s2:\n",
    "        meth_dat = pd.concat([meth_dat, pd.DataFrame({'Narrower':[r.strip()], 'Main':[s1[0].strip()], 'Main Label':[lab]})], sort=False)\n",
    "    return meth_dat\n",
    "\n",
    "\n",
    "def one_equals_others_minus_some(descr, lab):\n",
    "    #### D5A = D5 - (BR + CA + CN + IN + JP + RU + CH +US)\n",
    "    #meth_dat = pd.DataFrame(columns=['Narrower', 'Main'])\n",
    "    s1 = frame.split('=',1)\n",
    "    meth_dat = pd.DataFrame({'Narrower':['None'], 'Main':[s1[0].strip()], 'Main Label':[lab]})\n",
    "    return meth_dat\n",
    "\n",
    "\n",
    "def one_equals_one_and_minus_another(descr, lab):\n",
    "    #### Example = A3X=A3-MX\n",
    "    s1 = frame.split('=',1)\n",
    "    meth_dat = pd.DataFrame({'Narrower':['None'], 'Main':[s1[0].strip()], 'Main Label':[lab]})\n",
    "    return meth_dat\n",
    "\n",
    "\n",
    "def no_description(fram, lab, desc):   \n",
    "    return pd.DataFrame({'Narrower':['None'], 'Main':[fram.strip()], 'Main Label':[lab.strip()]})\n",
    "\n",
    "\n",
    "def all_pluses_in_description(descr, lab, fram):\n",
    "    #### Example = AG + BB + BS + BZ + CU + DM + DO + GD + GY + HT + JM + KN + LC + SR + TT + VC\n",
    "    meth_dat = pd.DataFrame(columns=['Narrower', 'Main', 'Main Label'])\n",
    "    s1 = descr.split('+')\n",
    "    for r in s1:\n",
    "        r = r.replace('(','')\n",
    "        meth_dat = pd.concat([meth_dat, pd.DataFrame({'Narrower':[r.strip()], 'Main':[fram.strip()], 'Main Label':[lab]})], sort=False)\n",
    "    return meth_dat\n",
    "\n",
    "\n",
    "def until_or_from_in_description(fram, lab, descr):\n",
    "    return no_description(fram, lab, descr)\n",
    "\n",
    "\n",
    "def all_minuses_in_description(fram, lab, descr):\n",
    "    return no_description(fram, lab, descr)\n",
    "\n",
    "\n",
    "def descr_has_from_until_upto(fram, lab, descr):\n",
    "    now = datetime.datetime.now()\n",
    "    yr = now.year\n",
    "    addTo = False    \n",
    "    unfro = pd.DataFrame(columns=['Narrower', 'Main', 'Main Label'])\n",
    "    if '=' in desc:\n",
    "        descr = descr.split('=',1)\n",
    "        descr = descr[1]\n",
    "    if '+' in descr:\n",
    "        s1 = descr.split('+')\n",
    "\n",
    "        for s in s1:\n",
    "            if 'from' in s.lower():\n",
    "                num = int((re.findall('\\d+', s))[0])\n",
    "                if num <= yr:\n",
    "                    s = ((s.split('(',1))[0]).strip()\n",
    "                    addTo = True\n",
    "                else:\n",
    "                    addTo = False\n",
    "            elif 'until' in s.lower():\n",
    "                num = int((re.findall('\\d+', s))[0])\n",
    "                if num >= yr:\n",
    "                    s = ((s.split('(',1))[0]).strip()\n",
    "                    addTo = True\n",
    "                else:\n",
    "                    addTo = False\n",
    "            elif 'up to' in s.lower():\n",
    "                num = int((re.findall('\\d+', s))[0])\n",
    "                if num >= yr:\n",
    "                    s = ((s.split('(',1))[0]).strip()\n",
    "                    addTo = True\n",
    "                else:\n",
    "                    addTo = False\n",
    "            else:\n",
    "                addTo = True\n",
    "                \n",
    "            if addTo:\n",
    "                unfro = pd.concat([unfro, pd.DataFrame({'Narrower':[s.strip()],'Main':[fram.strip()],\n",
    "                                'Main Label':[labe.strip()]})],sort=False)\n",
    "    else:\n",
    "        #print('Ignoring: ' + fram + ' === ' + desc)\n",
    "        unfro = pd.concat([unfro, pd.DataFrame({'Narrower':[''],'Main':[fram.strip()],\n",
    "                                'Main Label':[lab.strip()]})],sort=False)\n",
    "        \n",
    "    return unfro\n",
    "\n",
    "\n",
    "def another_special_case(fram, lab, descr):\n",
    "    # Descriptions with 'Country aggregate' or 'Candidate countries' in it\n",
    "    agg = pd.DataFrame(columns=['Narrower', 'Main', 'Main Label'])\n",
    "    s1 = descr.split(' (' , 1)\n",
    "    s1[1] = s1[1].replace(')','')\n",
    "    if '+' in s1[1]:\n",
    "        s2 = s1[1].split('+')\n",
    "    else:\n",
    "        s2 = s1[1].split(',')\n",
    "    for c in s2:\n",
    "        agg = pd.concat([agg, pd.DataFrame({'Narrower':[c.strip()],'Main':[fram.strip()],\n",
    "                        'Main Label':[lab.strip()]})],sort=False)\n",
    "    \n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.DataFrame(columns=['Narrower', 'Main', 'Main Label'])\n",
    "dat2 = pd.DataFrame(columns=['Narrower', 'Main', 'Main Label'])\n",
    "dat3 = pd.DataFrame(columns=['Narrower', 'Main', 'Main Label'])\n",
    "dat4 = pd.DataFrame(columns=['Narrower', 'Main', 'Main Label'])\n",
    "dat5 = pd.DataFrame(columns=['Narrower', 'Main', 'Main Label'])\n",
    "dat6 = pd.DataFrame(columns=['Narrower', 'Main', 'Main Label'])\n",
    "\n",
    "for item, frame in are['Notation'].iteritems():\n",
    "    desc = str((list(are['Description'][are['Notation'] == frame]))[0])\n",
    "    labe = str((list(are['Label'][are['Notation'] == frame]))[0])\n",
    "    desc = desc.strip().replace('  ', ' ')     \n",
    "    if ('aggregate' in labe.lower()) or ('candidate' in labe.lower()):\n",
    "        desc = 'special case'\n",
    "            \n",
    "    if (frame != 'A2A39') and (desc != 'nan'):\n",
    "        desc = desc.replace('W1=sum of all the (ISO 3166-1 alpha-2) partner countries +1A+W19;','')\n",
    "        desc = desc.replace('sum of (ISO 3166-1 alpha-2) partner countries;','')\n",
    "        desc = desc.replace('…','')\n",
    "        desc = desc.replace('+…','')\n",
    "        desc = desc.replace('(36 countries)','')\n",
    "        desc = desc.replace('+ A35E9','') # Need to look into this as A35E9 is not in Notations\n",
    "        desc = desc.replace('rule 1: ','')\n",
    "        desc = desc.replace('+ T F+','+ TF +')\n",
    "        if ('countries' in desc):\n",
    "            for x in range(90):\n",
    "                desc = desc.replace(f'({x} countries)','')  \n",
    "\n",
    "        if 'special case' in desc:\n",
    "            desc = labe\n",
    "            new_dat = another_special_case(frame, labe, desc)\n",
    "            dat6 = pd.concat([dat6,new_dat], sort=False)\n",
    "        elif ('from' in desc.lower()) or ('until' in desc.lower()) or ('up to' in desc.lower()):\n",
    "            new_dat = descr_has_from_until_upto(frame, labe, desc)\n",
    "            dat6 = pd.concat([dat6,new_dat], sort=False)\n",
    "            del new_dat\n",
    "        elif ('=' in desc) and ('-' not in desc):\n",
    "            new_dat = one_thing_equals_other_things(desc, labe)\n",
    "            dat1 = pd.concat([dat1,new_dat], sort=False)\n",
    "            del new_dat\n",
    "        elif ('=' in desc) and ('-' in desc) and ('from' not in desc):\n",
    "            if ('(' in desc) and (')' in desc):\n",
    "                if ('=' in desc) and (('+ (' in desc) or ('+(' in desc)) and ('-' in desc):\n",
    "                    new_dat = one_equals_others_plus_some_minus_some(desc, labe)\n",
    "                    dat2 = pd.concat([dat2,new_dat], sort=False)\n",
    "                    del new_dat\n",
    "                elif ('=' in desc) and (('- (' in desc) or ('-(' in desc)) and ('+' in desc):\n",
    "                    new_dat = one_equals_others_minus_some(desc, labe)\n",
    "                    dat2 = pd.concat([dat2,new_dat], sort=False)\n",
    "                    del new_dat\n",
    "                else:\n",
    "                    print(frame + ' -- ' + desc)\n",
    "            elif ('=' in desc) and ('-' in desc):\n",
    "                new_dat = one_equals_one_and_minus_another(desc, labe)\n",
    "                dat2 = pd.concat([dat2,new_dat], sort=False)\n",
    "                del new_dat\n",
    "            else:\n",
    "                print(frame + ' -- ' + desc)\n",
    "        elif 'rule' in desc:\n",
    "            new_dat = no_description(frame, labe, desc)\n",
    "            dat3 = pd.concat([dat3,new_dat], sort=False)\n",
    "            del new_dat\n",
    "        elif ('-' not in desc) and ('(' not in desc):\n",
    "            new_dat = all_pluses_in_description(desc, labe, frame)\n",
    "            dat5 = pd.concat([dat5,new_dat], sort=False)\n",
    "            del new_dat\n",
    "        elif '+' in desc:\n",
    "            new_dat = all_pluses_in_description(desc, labe, frame)\n",
    "            dat5 = pd.concat([dat5,new_dat], sort=False)\n",
    "            del new_dat\n",
    "        else:\n",
    "            new_dat = all_minuses_in_description(frame, labe, desc)\n",
    "            dat3 = pd.concat([dat3,new_dat], sort=False)\n",
    "            del new_dat\n",
    "    else:\n",
    "        new_dat = no_description(frame, labe, '')\n",
    "        dat4 = pd.concat([dat4,new_dat], sort=False)\n",
    "        del new_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of codes: 879\n",
      "dat1 count: 878 - Unique: 34\n",
      "dat2 count: 25 - Unique: 14\n",
      "dat3 count: 22 - Unique: 22\n",
      "dat4 count: 690 - Unique: 690\n",
      "dat5 count: 1571 - Unique: 90\n",
      "dat6 count: 395 - Unique: 29\n",
      "Before dropping duplicates from maindat: 879\n",
      "How many Main codes have been processed: 879\n",
      "How many Narrower codes: 364\n",
      "=============================================================================\n",
      "Number of Main Codes after joining: 879\n",
      "Number of Narrower codes after joining: 364\n"
     ]
    }
   ],
   "source": [
    "print('Original number of codes: ' + str(len(are['Notation'].unique())))\n",
    "print('dat1 count: ' + str(dat1['Main'].count()) + ' - Unique: ' + str(len(dat1['Main'].unique())))\n",
    "print('dat2 count: ' + str(dat2['Main'].count()) + ' - Unique: ' + str(len(dat2['Main'].unique())))\n",
    "print('dat3 count: ' + str(dat3['Main'].count()) + ' - Unique: ' + str(len(dat3['Main'].unique())))\n",
    "print('dat4 count: ' + str(dat4['Main'].count()) + ' - Unique: ' + str(len(dat4['Main'].unique())))\n",
    "print('dat5 count: ' + str(dat5['Main'].count()) + ' - Unique: ' + str(len(dat5['Main'].unique())))\n",
    "print('dat6 count: ' + str(dat6['Main'].count()) + ' - Unique: ' + str(len(dat6['Main'].unique())))\n",
    "dat1 = dat1.drop_duplicates()\n",
    "dat2 = dat2.drop_duplicates()\n",
    "dat3 = dat3.drop_duplicates()\n",
    "dat4 = dat4.drop_duplicates()\n",
    "dat5 = dat5.drop_duplicates()\n",
    "dat6 = dat6.drop_duplicates()\n",
    "maindat = pd.concat([dat1, dat2, dat3, dat4, dat5, dat6])\n",
    "print('Before dropping duplicates from maindat: ' + str(len(maindat['Main'].unique())))\n",
    "maindat = maindat.drop_duplicates()\n",
    "print('How many Main codes have been processed: ' + str(len(maindat['Main'].unique())))\n",
    "print('How many Narrower codes: ' + str(len(maindat['Narrower'].unique())))\n",
    "# Merge on Main to get the description\n",
    "maindat3 = pd.merge(maindat, are, how='left', left_on=['Main'], right_on=['Notation'])\n",
    "maindat3 = maindat3.rename(columns={'Description':'Main Description'})\n",
    "maindat3 = maindat3[['Narrower','Main','Main Label','Main Description']]\n",
    "# Merge on Narrower to get the label and description\n",
    "maindat3 = pd.merge(maindat3, are, how='left', left_on=['Narrower'], right_on=['Notation'])\n",
    "maindat3 = maindat3.rename(columns={'Description':'Narrow Description', 'Label':'Narrow Label'})\n",
    "maindat3 = maindat3[['Narrow Description','Narrow Label','Narrower','Main','Main Label','Main Description']]\n",
    "print('=============================================================================')\n",
    "print('Number of Main Codes after joining: ' + str(len(maindat3['Main'].unique())))\n",
    "print('Number of Narrower codes after joining: ' + str(len(maindat3['Narrower'].unique())))\n",
    "maindat = maindat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dat1, dat2, dat3, dat4, dat5, dat6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set things to None rather than NaN cos i don't like NaNs\n",
    "for c in maindat.columns:\n",
    "    maindat[c][maindat[c].isna()] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Notation</th>\n",
       "      <th>Main</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Notation, Main]\n",
       "Index: []"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if all codes have been captured\n",
    "df = pd.merge(are, maindat, how='left', left_on='Notation', right_on='Main')\n",
    "df = df[['Notation','Main']]\n",
    "d = df[df['Main'].isna()]\n",
    "# If nothing comes our of this then everything SHOULD be alright\n",
    "d.head(60)\n",
    "#maindat[maindat['Main'].isin(['U30'])].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "repoBase = \"http://gss-data.org.uk/def\"\n",
    "con = \"concept\"\n",
    "sch = \"scheme\"\n",
    "consch = con + \"-\" + sch\n",
    "sortPri = 'http://www.w3.org/ns/ui#sortPriority'\n",
    "\n",
    "mainLabel = \"SDMX Area codelist\"\n",
    "main =\"cl-area\"\n",
    "desc = \"Dataset representing the SDMX Area codelist. URN: urn:sdmx:org.sdmx.infomodel.codelist.Codelist=IMF:CL_AREA(1.13). Main source: https://sdmx.org/?page_id=1747. Structure source: https://registry.sdmx.org/ws/public/sdmxapi/rest/datastructure/IMF/BOP/1.14/?detail=full&references=descendants&version=2.1:1:555:\"\n",
    "\n",
    "schemeURI = URIRef(repoBase + \"/\" + con + '-' + sch + \"/\" + main)\n",
    "i = 1\n",
    "\n",
    "datasetStr = '/dataset'\n",
    "catrecStr = '/catalog-record'\n",
    "# Set up the main schema triples\n",
    "g.add((schemeURI, RDF.type, SKOS.ConceptScheme))\n",
    "g.add((schemeURI, RDF.type, URIRef(\"http://publishmydata.com/pmdcat#ConceptScheme\")))\n",
    "g.add((schemeURI, URIRef(\"http://purl.org/dc/terms/title\"), Literal(mainLabel, lang=\"en\")))\n",
    "g.add((schemeURI, RDFS.label, Literal(mainLabel, lang=\"en\")))\n",
    "g.add((schemeURI, RDFS.seeAlso, URIRef(schemeURI + datasetStr)))\n",
    "g.add((schemeURI, URIRef(\"http://www.w3.org/ns/prov#hadDerivation\"), URIRef(schemeURI)))\n",
    "g.add((schemeURI, URIRef(\"http://gss-data.org.uk/catalog/vocabularies\"), URIRef(schemeURI + catrecStr)))\n",
    " \n",
    "datasetDateTime = \"2021-03-04T11:40:13Z\"\n",
    "g.add((URIRef(schemeURI + datasetStr), RDF.type, URIRef(\"http://www.w3.org/ns/dcat#Dataset\")))\n",
    "g.add((URIRef(schemeURI + datasetStr), RDF.type, URIRef(\"http://publishmydata.com/pmdcat#Dataset\")))\n",
    "g.add((URIRef(schemeURI + datasetStr), URIRef(\"http://purl.org/dc/terms/modified\"), Literal(datasetDateTime, datatype=XSD.dateTime)))\n",
    "g.add((URIRef(schemeURI + datasetStr), URIRef(\"http://publishmydata.com/pmdcat#graph\"), URIRef(schemeURI)))\n",
    "g.add((URIRef(schemeURI + datasetStr), URIRef(\"http://purl.org/dc/terms/issued\"), Literal(datasetDateTime, datatype=XSD.dateTime)))\n",
    "g.add((URIRef(schemeURI + datasetStr), RDFS.label, Literal(mainLabel)))\n",
    "g.add((URIRef(schemeURI + datasetStr), URIRef(\"http://publishmydata.com/pmdcat#datasetContents\"), URIRef(schemeURI)))\n",
    "g.add((URIRef(schemeURI + datasetStr), RDFS.comment, Literal(desc, lang=\"en\")))\n",
    "g.add((URIRef(schemeURI + datasetStr), URIRef(\"http://purl.org/dc/terms/title\"), Literal(mainLabel)))\n",
    "\n",
    "\n",
    "g.add((URIRef(\"http://gss-data.org.uk/catalog/vocabularies\"), URIRef(\"http://www.w3.org/ns/dcat#record\"), URIRef(schemeURI + catrecStr)))\n",
    "\n",
    "catRecDateTime = \"2020-12-23T11:58:58.120533\"\n",
    "g.add((URIRef(schemeURI + catrecStr), RDF.type, URIRef(\"http://www.w3.org/ns/dcat#CatalogRecord\")))\n",
    "g.add((URIRef(schemeURI + catrecStr), URIRef(\"http://purl.org/dc/terms/title\"), Literal(mainLabel, lang=\"en\")))\n",
    "g.add((URIRef(schemeURI + catrecStr), URIRef(\"http://purl.org/dc/terms/issued\"), Literal(catRecDateTime, datatype=XSD.dateTime)))\n",
    "g.add((URIRef(schemeURI + catrecStr), URIRef(\"http://xmlns.com/foaf/0.1/primaryTopic\"), URIRef(schemeURI + '/dataset')))\n",
    "g.add((URIRef(schemeURI + catrecStr), URIRef(\"http://purl.org/dc/terms/modified\"), Literal(catRecDateTime, datatype=XSD.dateTime)))\n",
    "g.add((URIRef(schemeURI + catrecStr), URIRef(\"http://publishmydata.com/pmdcat#metadataGraph\"), URIRef(schemeURI)))\n",
    "\n",
    "\n",
    "for row in maindat.iterrows():\n",
    "    if 'None' not in row[1]['Main']:\n",
    "        conceptURI = URIRef(repoBase + \"/\" + con + \"/\" + main + \"/\" + str(pathify(row[1]['Main'])).upper())\n",
    "        g.add((conceptURI, RDF.type, SKOS.Concept))\n",
    "        g.add((conceptURI, RDFS.label, Literal(row[1]['Main Label'], lang=\"en\")))\n",
    "        g.add((conceptURI, SKOS.prefLabel, Literal(row[1]['Main Label'], lang=\"en\")))\n",
    "        g.add((conceptURI, SKOS.altLabel, Literal(row[1]['Main Label'] + \" - \" + row[1]['Main'], lang=\"en\")))\n",
    "        g.add((conceptURI, SKOS.inScheme, URIRef(schemeURI)))\n",
    "        g.add((conceptURI, SKOS.notation, Literal(str(pathify(row[1]['Main'])).upper())))\n",
    "        if 'None' not in row[1]['Narrower']:\n",
    "            broaderURI = URIRef(repoBase + \"/\" + con + \"/\" + main + \"/\" + str(pathify(row[1]['Narrower']).upper()))\n",
    "            g.add((conceptURI, SKOS.Narrower, broaderURI))\n",
    "        try:\n",
    "            if 'None' not in row[1]['Main Description']:\n",
    "                g.add((conceptURI, RDFS.comment, Literal(row[1]['Main Description'], lang=\"en\")))\n",
    "        except:\n",
    "            #print(row[1]['Main Description'])\n",
    "            i = 0\n",
    "#        #g.add((conceptURI, URIRef(sortPri), Literal(i)))\n",
    "#        i = i + 1   \n",
    "    if 'None' not in row[1]['Narrower']:\n",
    "        conceptURI = URIRef(repoBase + \"/\" + con + \"/\" + main + \"/\" + str(pathify(row[1]['Narrower'])).upper())\n",
    "        g.add((conceptURI, RDF.type, SKOS.Concept))\n",
    "        g.add((conceptURI, RDFS.label, Literal(row[1]['Narrow Label'], lang=\"en\")))\n",
    "        g.add((conceptURI, SKOS.prefLabel, Literal(row[1]['Narrow Label'], lang=\"en\")))\n",
    "        g.add((conceptURI, SKOS.altLabel, Literal(row[1]['Narrow Label'] + \" - \" + row[1]['Narrower'], lang=\"en\")))\n",
    "        g.add((conceptURI, SKOS.inScheme, URIRef(schemeURI)))\n",
    "        g.add((conceptURI, SKOS.notation, Literal(str(pathify(row[1]['Narrower'])).upper())))\n",
    "        if 'None' not in row[1]['Main']:\n",
    "            broaderURI = URIRef(repoBase + \"/\" + con + \"/\" + main + \"/\" + str(pathify(row[1]['Main']).upper()))\n",
    "            g.add((conceptURI, SKOS.broader, broaderURI))\n",
    "        try:\n",
    "            if 'None' not in row[1]['Narrow Description']:\n",
    "                g.add((conceptURI, RDFS.comment, Literal(row[1]['Narrow Description'], lang=\"en\")))\n",
    "        except:\n",
    "            #print(row[1]['Narrow Description'])\n",
    "            i = 0\n",
    "        #g.add((conceptURI, URIRef(sortPri), Literal(i)))\n",
    "        i = i + 1   \n",
    "\n",
    "#g.bind(\"cogs\", schemeURI + \"/\")\n",
    "g.bind(\"skos\", SKOS)\n",
    "g.bind(\"rdf\", RDF)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"dc\", URIRef(\"http://purl.org/dc/terms/\"))\n",
    "g.bind(\"prov\", URIRef(\"http://www.w3.org/ns/prov#\"))\n",
    "g.bind(\"vocab\", URIRef(\"http://gss-data.org.uk/catalog/\"))\n",
    "g.bind(\"pmdcat\", URIRef(\"http://publishmydata.com/pmdcat#\"))\n",
    "g.bind(\"dcat\", URIRef(\"http://www.w3.org/ns/dcat#\"))\n",
    "g.bind(\"foaf\", URIRef(\"http://xmlns.com/foaf/0.1/\"))\n",
    "#print(g.serialize(format='n3').decode(\"utf-8\"))\n",
    "\n",
    "g.serialize(destination='cl-area.ttl', format='turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to get rid of the @prefixs in th output file, probably can do this within RDFLIB but i'm doing it this way!!\n",
    "repl = {  \n",
    "\"vocab:vocabularies\": \"<http://gss-data.org.uk/catalog/vocabularies>\",\n",
    "\"rdfs:label\": \"<http://www.w3.org/2000/01/rdf-schema#label>\",\n",
    "\"rdfs:seeAlso\": \"<http://www.w3.org/2000/01/rdf-schema#seeAlso>\",\n",
    "\"rdfs:comment\": \"<http://www.w3.org/2000/01/rdf-schema#comment>\",\n",
    "\"skos:ConceptScheme\": \"<http://www.w3.org/2004/02/skos/core#ConceptScheme>\",\n",
    "\"skos:Concept\": \"<http://www.w3.org/2004/02/skos/core#Concept>\",\n",
    "\"skos:altLabel\": \"<http://www.w3.org/2004/02/skos/core#altLabel>\",\n",
    "\"skos:inScheme\": \"<http://www.w3.org/2004/02/skos/core#inScheme>\",\n",
    "\"skos:notation\": \"<http://www.w3.org/2004/02/skos/core#notation>\",\n",
    "\"skos:prefLabel\": \"<http://www.w3.org/2004/02/skos/core#prefLabel>\",\n",
    "\"skos:broader\": \"<http://www.w3.org/2004/02/skos/core#broader>\",\n",
    "\"skos:Narrower\": \"<http://www.w3.org/2004/02/skos/core#narrower>\",\n",
    "\"dcat:record\": \"<http://www.w3.org/ns/dcat#record>\",\n",
    "\"dcat:CatalogRecord\": \"<http://www.w3.org/ns/dcat#CatalogRecord>\",\n",
    "\"dcat:Dataset\": \"<http://www.w3.org/ns/dcat#dataset>\",\n",
    "\"dc:issued\": \"<http://purl.org/dc/terms/issued>\",\n",
    "\"dc:modified\": \"<http://purl.org/dc/terms/modified>\",\n",
    "\"dc:title\": \"<http://purl.org/dc/terms/title>\",\n",
    "\"xsd:dateTime\": \"<http://www.w3.org/2001/XMLSchema#dateTime>\",\n",
    "\"foaf:primaryTopic\": \"<http://xmlns.com/foaf/0.1/primaryTopic>\",\n",
    "\"pmdcat:metadataGraph\": \"<http://publishmydata.com/pmdcat#metadataGraph>\",\n",
    "\"pmdcat:Dataset\": \"<http://publishmydata.com/pmdcat#Dataset>\",\n",
    "\"pmdcat:datasetContents\": \"<http://publishmydata.com/pmdcat#datasetContents>\",\n",
    "\"pmdcat:graph\": \"<http://publishmydata.com/pmdcat#graph>\",\n",
    "\"pmdcat:ConceptScheme\": \"<http://publishmydata.com/pmdcat#ConceptScheme>\",\n",
    "\"xsd:dateTime\": \"<http://www.w3.org/2001/XMLSchema#dateTime>\",\n",
    "\"prov:hadDerivation\": \"<http://www.w3.org/ns/prov#hadDerivation>\"\n",
    "}\n",
    "\n",
    "pref = \"\"\"@prefix dc: <http://purl.org/dc/terms/> .\n",
    "@prefix dcat: <http://www.w3.org/ns/dcat#> .\n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
    "@prefix pmdcat: <http://publishmydata.com/pmdcat#> .\n",
    "@prefix prov: <http://www.w3.org/ns/prov#> .\n",
    "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix skos: <http://www.w3.org/2004/02/skos/core#> .\n",
    "@prefix vocab: <http://gss-data.org.uk/catalog/> .\n",
    "@prefix xml: <http://www.w3.org/XML/1998/namespace> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"cl-area.ttl\", \"r\")\n",
    "txt = f.read()\n",
    "f.close\n",
    "for a in repl:\n",
    "    txt = txt.replace(str(a),repl[a])\n",
    "\n",
    "txt = txt.replace(pref,'')\n",
    "txt = txt.strip()\n",
    "f = open(\"cl-area.ttl\", \"w\")\n",
    "f.write(txt)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ALL OLD DEVELOPMENT CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "are.head(10)\n",
    "a = are[~are['Description'].isna()]\n",
    "a = a[['Description','Label','Notation']]\n",
    "u = a[a['Description'].str.contains('until')]\n",
    "f = a[a['Description'].str.contains('from')]\n",
    "x = a[a['Description'].str.contains('up to')]\n",
    "awk = pd.concat([u,f,x])\n",
    "awk = awk.drop_duplicates()\n",
    "print(awk['Label'].count())\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "yr = now.year\n",
    "addTo = False    \n",
    "unfro = pd.DataFrame(columns=['Narrower','Main','Main Label','Main Description'])\n",
    "for item, frame in awk['Description'].iteritems():\n",
    "\n",
    "    labe = str((list(awk['Label'][awk['Description'] == frame]))[0])\n",
    "    note = str((list(awk['Notation'][awk['Description'] == frame]))[0])\n",
    "    if '+' in frame:\n",
    "        s1 = frame.split('+')\n",
    "        for s in s1:\n",
    "            if 'from' in s.lower():\n",
    "                num = int((re.findall('\\d+', s))[0])\n",
    "                if num <= yr:\n",
    "                    s = ((s.split('(',1))[0]).strip()\n",
    "                    addTo = True\n",
    "                else:\n",
    "                    addTo = False\n",
    "            elif 'until' in s.lower():\n",
    "                num = int((re.findall('\\d+', s))[0])\n",
    "                if num >= yr:\n",
    "                    s = ((s.split('(',1))[0]).strip()\n",
    "                    addTo = True\n",
    "                else:\n",
    "                    addTo = False\n",
    "            elif 'up to' in s.lower():\n",
    "                num = int((re.findall('\\d+', s))[0])\n",
    "                if num >= yr:\n",
    "                    s = ((s.split('(',1))[0]).strip()\n",
    "                    addTo = True\n",
    "                else:\n",
    "                    addTo = False\n",
    "            else:\n",
    "                addTo = True\n",
    "            #addTo = True\n",
    "            if addTo:\n",
    "                unfro = pd.concat([unfro, pd.DataFrame({'Narrower':[s.strip()],'Main':[note.strip()],\n",
    "                               'Main Label':[labe.strip()],'Main Description':[frame.strip()]})],sort=False)\n",
    "    #break\n",
    "    else:\n",
    "        print(note + ' === ' + frame)\n",
    "        \n",
    "unfro = unfro.drop_duplicates()\n",
    "print(unfro['Main'].count())\n",
    "#unfro.head(60)\n",
    "unfro['Narrower'].unique()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Have mamanually created some rows for the ones with awkward descriptions with the words \"until\" or \"from\"\n",
    "# 1 = Original code and label and aURL form where i got information about the code\n",
    "# 2 = Original code and desription from the cl-area file\n",
    "# 3 = Original code with the altered description, which describes the latest make up of the countries based ont he until and from statements\n",
    "# n = made up codes based on previous incarnation sof the countries based on the from and until statements ({original code}-{year})\n",
    "awk = pd.DataFrame(columns=['Narrower','Main'])\n",
    "with open('until_description.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line != '':\n",
    "            splitline = line.split('..',1)\n",
    "            l = splitline[1].split('*', 1)\n",
    "            if ('Label' not in l[1]) and ('Original description' not in l[1]):\n",
    "                try:\n",
    "                        # Line 3 has the original code the others are made up so ignoring for now\n",
    "                    if '3' in splitline[0]:\n",
    "                        a = l[0].split('=',1)\n",
    "                        b = a[1].split('+')\n",
    "                        for c in b:\n",
    "                            awk = pd.concat([awk, pd.DataFrame({'Narrower':[c.strip()],'Main':[a[0].strip()]})],sort=False)\n",
    "                except:\n",
    "                    i = 0\n",
    "\n",
    "awk2 = pd.merge(awk, are, how='left', left_on=['Main'], right_on=['Notation'])\n",
    "awk2 = awk2[['Narrower','Main','Label','Description']]\n",
    "awk2 = awk2.rename(columns={'Label':'Main Label', 'Description':'Main Description'})\n",
    "awk2 = pd.merge(awk2, are, how='left', left_on=['Narrower'], right_on=['Notation'])\n",
    "awk2 = awk2[['Description','Label','Narrower','Main','Main Label','Main Description']]\n",
    "awk2 = awk2.rename(columns={'Label':'Narrow Label', 'Description':'Narrow Description'})\n",
    "awk2['Narrow Description'][awk2['Narrow Description'].isna()] = 'None'\n",
    "maindat = maindat[~maindat['Main'].isin(awk2['Main'].unique())]\n",
    "maindat = pd.concat([maindat,awk2])\n",
    "maindat['Narrower'][maindat['Narrower'].isna()] = 'None'\n",
    "maindat['Narrower'][maindat['Narrower'] == ''] = 'None'\n",
    "maindat.head(60)\n",
    "maindat = maindat.drop_duplicates()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "agg = pd.DataFrame(columns=['Narrower','Main', 'Main Label'])\n",
    "ca1 = are[are['Label'].str.contains('Country aggregate')]\n",
    "ca2 = are[are['Label'].str.contains('Candidate countries')]\n",
    "ca = pd.concat([ca1,ca2])\n",
    "for item, frame in ca['Label'].iteritems():\n",
    "    cod = str((list(ca['Notation'][ca['Label'] == frame]))[0])\n",
    "    s1 = frame.split(' (' , 1)\n",
    "    s1[1] = s1[1].replace(')','')\n",
    "    if '+' in s1[1]:\n",
    "        s2 = s1[1].split('+')\n",
    "    else:\n",
    "        s2 = s1[1].split(',')\n",
    "    for c in s2:\n",
    "        agg = pd.concat([agg, pd.DataFrame({'Narrower':[c.strip()],'Main':[cod.strip()],\n",
    "                        'Main Label':[frame]})],sort=False)\n",
    "\n",
    "agg = pd.merge(agg, are, how='left', left_on=['Narrower'], right_on=['Notation'])\n",
    "agg = agg.rename(columns={'Label':'Narrow Label', 'Description':'Narrow Description'})\n",
    "agg = agg[['Narrow Description', 'Narrow Label','Narrower','Main','Main Label']]\n",
    "print(maindat['Main'].count())\n",
    "maindat = maindat[~maindat['Main'].isin(agg['Main'].unique())]\n",
    "print(maindat['Main'].count())\n",
    "maindat = pd.concat([maindat,agg])\n",
    "print(maindat['Main'].count())\n",
    "agg.head(20)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "print(now.year, now.month, now.day, now.hour, now.minute, now.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitb3249c1df48246aabe11cf3e801b18c0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
